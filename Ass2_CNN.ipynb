{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from numpy import argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import nltk\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_data = pd.read_csv('./Training Dataset-20191010/labeled_data.csv')\n",
    "unlabeled_data = pd.read_csv('./Training Dataset-20191010/unlabeled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    The new rule is - if you are waiting for a tab...\n",
       "1    Flirted with giving this two stars, but that's...\n",
       "2    I was staying at planet Hollywood across the s...\n",
       "3    Food is good but prices are super expensive. 8...\n",
       "4    Worse company to deal with they do horrible wo...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(lab_data['text']).split()).value_counts()[-10:]\n",
    "freq\n",
    "freq = list(freq.index)\n",
    "lab_data['text'] = lab_data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "lab_data['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Normalisation, Tokenization and most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Had a good experience when my wife and I sat a...\n",
       "1    On my first to Montreal with my gf we came her...\n",
       "2    One of our favorite places to go when it's col...\n",
       "3    The doctor was very nice, got in in a good amo...\n",
       "4    The Nook is an immediate phoenix staple! I cam...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(' '.join(unlabeled_data['text']).split()).value_counts()[-10:]\n",
    "freq\n",
    "freq = list(freq.index)\n",
    "unlabeled_data['text'] = unlabeled_data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "unlabeled_data['text'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_characters(raw_text):\n",
    "    processed_text = re.sub('\\\\n','', raw_text)\n",
    "    processed_text = re.sub('\\\\r','', processed_text)\n",
    "    processed_text = re.sub(\"\\\\'\", \"\\'\",processed_text)\n",
    "    processed_text = re.sub(r'\\d+','', processed_text)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_data['text'] = lab_data.apply(lambda row: remove_extra_characters(row['text'].strip()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_data['text'] = unlabeled_data.apply(lambda row: remove_extra_characters(row['text'].strip()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(token_list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_token = []\n",
    "    for each in token_list :\n",
    "#         print(each ,\":\", lemmatizer.lemmatize(each)) \n",
    "        lem_token.append(lemmatizer.lemmatize(each))\n",
    "    return lem_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"\\w+(?:[']\\w+)?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(raw_data):\n",
    "    raw_data1 = raw_data.lower()\n",
    "    tokenised = tokenizer.tokenize(raw_data1)\n",
    "#     tokenised = nltk.tokenize.word_tokenize(raw_data1)\n",
    "    #lem_token = lemmatization(tokenised)\n",
    "#     stopwords_tokens = [w for w in tokenised if not w in stopwords]\n",
    "    #processed_data = ' '.join(lem_token)\n",
    "    processed_data = ' '.join(tokenised)\n",
    "        \n",
    "    return(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lab_data['text'] = lab_data.apply(lambda row: token(row['text'].strip()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_data['text'] = unlabeled_data.apply(lambda row: token(row['text'].strip()), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase = True,analyzer = 'word',ngram_range = (1,2), min_df=3, max_df=.99)\n",
    "    \n",
    "train_review = vectorizer.fit_transform(lab_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lab_data['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_review, lab_data['label'],test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                2374850   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 66        \n",
      "=================================================================\n",
      "Total params: 2,374,916\n",
      "Trainable params: 2,374,916\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]  # Number of features\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(6, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "               optimizer='adam', \n",
    "               metrics=['accuracy'])\n",
    "model.summary()\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "cnn_model = model.fit(X_train, y_train,\n",
    "                     epochs=2,\n",
    "                    verbose=False,\n",
    "                     validation_data=(X_test, y_test),\n",
    "                     batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(y_true, y_pred):\n",
    "    return np.equal(np.argmax(y_true, axis=-1), np.argmax(y_pred, axis=-1)).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6274\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy: \" + str(acc(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_unlabeled = unlabeled_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_test = vectorizer.transform(remaining_unlabeled['text'])\n",
    "# pred_class = model.predict(unlabeled_test)\n",
    "# pred_probab = model.predict_proba(unlabeled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_class = model.predict(unlabeled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pred = []\n",
    "for i in range(len(pred_class)):\n",
    "    p_pred.append(argmax(pred_class[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probab = model.predict_proba(unlabeled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test = []\n",
    "for i in range(len(pred_probab)):\n",
    "    p_test.append(max(pred_probab[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_unlabeled['label'] = p_pred\n",
    "remaining_unlabeled['probability'] = p_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_data = remaining_unlabeled[remaining_unlabeled['probability'] > 0.95]\n",
    "print(\"length of obtained train data:\", len(new_train_data))\n",
    "    \n",
    "remaining_unlabeled = remaining_unlabeled[remaining_unlabeled['probability'] <= 0.95]\n",
    "print(\"length of remaining data:\", len(remaining_unlabeled))\n",
    "    \n",
    "new_train_data.drop(['probability'], axis=1, inplace=True)\n",
    "remaining_unlabeled.drop(['probability','label'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([lab_data, new_train_data])\n",
    "print(\"length of train data:\", len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(lowercase = True,analyzer = 'word',ngram_range = (1,2), min_df=3, max_df=.99)\n",
    "train = vectorizer.fit_transform(train_data['text'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, train_data['label'],test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]  # Number of features\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(6, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "               optimizer='adam', \n",
    "               metrics=['accuracy'])\n",
    "model.summary()\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "model.fit(X_train, y_train,\n",
    "                     epochs=2,\n",
    "                    verbose=False,\n",
    "                     validation_data=(X_test, y_test),\n",
    "                     batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "    \n",
    "print(\"accuracy: \" + str(acc(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_unlabeled = unlabeled_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of obtained train data: 9989\n",
      "length of remaining data: 590011\n",
      "length of train data: 59989\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 10)                2814850   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 66        \n",
      "=================================================================\n",
      "Total params: 2,814,916\n",
      "Trainable params: 2,814,916\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "accuracy: 0.6840306717786298\n"
     ]
    }
   ],
   "source": [
    "for j in range(1):\n",
    "    unlabeled_test = vectorizer.transform(remaining_unlabeled['text'])\n",
    "    pred_class = model.predict(unlabeled_test)\n",
    "    \n",
    "    p_pred = []\n",
    "    for i in range(len(pred_class)):\n",
    "        p_pred.append(argmax(pred_class[i]))\n",
    "        \n",
    "    pred_probab = model.predict_proba(unlabeled_test)\n",
    "    \n",
    "    p_test = []\n",
    "    \n",
    "    for i in range(len(pred_probab)):\n",
    "        p_test.append(max(pred_probab[i]))\n",
    "    \n",
    "    \n",
    "    remaining_unlabeled['label'] = p_pred\n",
    "    remaining_unlabeled['probability'] = p_test\n",
    "    \n",
    "    new_train_data = remaining_unlabeled[remaining_unlabeled['probability'] > 0.95]\n",
    "    print(\"length of obtained train data:\", len(new_train_data))\n",
    "    \n",
    "    remaining_unlabeled = remaining_unlabeled[remaining_unlabeled['probability'] <= 0.95]\n",
    "    print(\"length of remaining data:\", len(remaining_unlabeled))\n",
    "    \n",
    "    new_train_data.drop(['probability'], axis=1, inplace=True)\n",
    "    remaining_unlabeled.drop(['probability','label'], axis=1, inplace=True)\n",
    "    \n",
    "    if j == 0:\n",
    "        train_data = pd.concat([lab_data, new_train_data])\n",
    "        print(\"length of train data:\", len(train_data))\n",
    "    else:\n",
    "        td = train_data.copy()\n",
    "        train_data = pd.concat([td, new_train_data])\n",
    "        print(\"length of train data:\", len(train_data))\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(lowercase = True,analyzer = 'word',ngram_range = (1,2), min_df=3, max_df=.99)\n",
    "    train = vectorizer.fit_transform(train_data['text'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train, train_data['label'],test_size=0.20, random_state=1)\n",
    "    \n",
    "    \n",
    "    input_dim = X_train.shape[1]  # Number of features\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "    model.add(layers.Dense(6, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "               optimizer='adam', \n",
    "               metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    model.fit(X_train, y_train,\n",
    "                     epochs=2,\n",
    "                    verbose=False,\n",
    "                     validation_data=(X_test, y_test),\n",
    "                     batch_size=30)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(\"accuracy: \" + str(acc(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"./test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_1</td>\n",
       "      <td>trying to have a nice quiet dinner.  the annou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_2</td>\n",
       "      <td>Been getting food to go from here for over 3yr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_3</td>\n",
       "      <td>Ugh. I've had to eat here a couple of times be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_4</td>\n",
       "      <td>The people here are so nice! I ordered on eat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_5</td>\n",
       "      <td>Heard alot of good things about this place and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  test_id                                               text\n",
       "0  test_1  trying to have a nice quiet dinner.  the annou...\n",
       "1  test_2  Been getting food to go from here for over 3yr...\n",
       "2  test_3  Ugh. I've had to eat here a couple of times be...\n",
       "3  test_4  The people here are so nice! I ordered on eat ...\n",
       "4  test_5  Heard alot of good things about this place and..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['text'] = test_data.apply(lambda row: remove_extra_characters(row['text'].strip()), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['text'] = test_data.apply(lambda row: token(row['text'].strip()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vectorizer.transform(test_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_class = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pred = []\n",
    "for i in range(len(pred_class)):\n",
    "    p_pred.append(argmax(pred_class[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = pd.DataFrame({'test_id':test_data['test_id'], 'label':p_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data.to_csv(\"predict_label_cnn_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
