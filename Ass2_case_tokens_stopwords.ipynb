{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import nltk\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import gensim\n",
    "import logging\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import word2vec\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import MWETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_data = pd.read_csv('./Training Dataset-20191010/labeled_data.csv')\n",
    "# unlabeled_data = pd.read_csv('./Training Dataset-20191010/unlabeled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# lab_data.head()['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Flirted with giving this two stars, but that\\'s a pretty damning rating for what might have just been an off night...\\r\\n\\r\\nNew to the East side, and so we don\\'t know many of these hidden gems, but me and the fiance met her friend for drinks here and ended up getting some things to nibble. \\r\\n\\r\\nFirst off, service was pretty slow, which was unusual because the restaurant is pretty small and galley style. You would think it would be easy for servers to routinely hit up tables as you pass by. \\r\\n\\r\\nThe fiance ordered the Quinoa Salad, and said it was pretty good, but dry. I wasn\\'t too hungry and so I simply ordered the Bruchetta 3-way which came with burnt crostinis. And I ordered a side of fries, which were either hard or chewy.\\r\\n\\r\\nThe friend ordered the macaroni & cheese, and added chicken and bacon (her usual order) and liked it.  \\r\\n\\r\\nCan\\'t remember the last time I thought to myself- \"Huh... they failed at fries...\" So, like I said- two stars. But, the decor was good, it was a good place to have a conversation, and I might be back to try more expensive fare, but-... ah... the fry thing... yeeesh... I dunno, man...'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_data['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_characters(raw_text):\n",
    "    processed_text = re.sub('\\\\n','', raw_text)\n",
    "    processed_text = re.sub('\\\\r','', processed_text)\n",
    "    processed_text = re.sub(\"\\\\'\", \"\\'\",processed_text)\n",
    "    processed_text = re.sub(r'\\d+','', processed_text)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_data['text'] = lab_data.apply(lambda row: remove_extra_characters(row['text'].strip()), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Normalisation, Tokenization and Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "with open('./stopwords_en.txt') as f:\n",
    "    stopwords = f.read().splitlines()\n",
    "stopwords = set(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " \"a's\",\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'across',\n",
       " 'actually',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " \"ain't\",\n",
       " 'all',\n",
       " 'allow',\n",
       " 'allows',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'apart',\n",
       " 'appear',\n",
       " 'appreciate',\n",
       " 'appropriate',\n",
       " 'are',\n",
       " \"aren't\",\n",
       " 'around',\n",
       " 'as',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asking',\n",
       " 'associated',\n",
       " 'at',\n",
       " 'available',\n",
       " 'away',\n",
       " 'awfully',\n",
       " 'b',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'believe',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'better',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'brief',\n",
       " 'but',\n",
       " 'by',\n",
       " 'c',\n",
       " \"c'mon\",\n",
       " \"c's\",\n",
       " 'came',\n",
       " 'can',\n",
       " \"can't\",\n",
       " 'cannot',\n",
       " 'cant',\n",
       " 'cause',\n",
       " 'causes',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'changes',\n",
       " 'clearly',\n",
       " 'co',\n",
       " 'com',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'concerning',\n",
       " 'consequently',\n",
       " 'consider',\n",
       " 'considering',\n",
       " 'contain',\n",
       " 'containing',\n",
       " 'contains',\n",
       " 'corresponding',\n",
       " 'could',\n",
       " \"couldn't\",\n",
       " 'course',\n",
       " 'currently',\n",
       " 'd',\n",
       " 'definitely',\n",
       " 'described',\n",
       " 'despite',\n",
       " 'did',\n",
       " \"didn't\",\n",
       " 'different',\n",
       " 'do',\n",
       " 'does',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " \"don't\",\n",
       " 'done',\n",
       " 'down',\n",
       " 'downwards',\n",
       " 'during',\n",
       " 'e',\n",
       " 'each',\n",
       " 'edu',\n",
       " 'eg',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'enough',\n",
       " 'entirely',\n",
       " 'especially',\n",
       " 'et',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everybody',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'ex',\n",
       " 'exactly',\n",
       " 'example',\n",
       " 'except',\n",
       " 'f',\n",
       " 'far',\n",
       " 'few',\n",
       " 'fifth',\n",
       " 'first',\n",
       " 'five',\n",
       " 'followed',\n",
       " 'following',\n",
       " 'follows',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forth',\n",
       " 'four',\n",
       " 'from',\n",
       " 'further',\n",
       " 'furthermore',\n",
       " 'g',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'go',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'got',\n",
       " 'gotten',\n",
       " 'greetings',\n",
       " 'h',\n",
       " 'had',\n",
       " \"hadn't\",\n",
       " 'happens',\n",
       " 'hardly',\n",
       " 'has',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he's\",\n",
       " 'hello',\n",
       " 'help',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " \"here's\",\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'hi',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'hither',\n",
       " 'hopefully',\n",
       " 'how',\n",
       " 'howbeit',\n",
       " 'however',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'ie',\n",
       " 'if',\n",
       " 'ignored',\n",
       " 'immediate',\n",
       " 'in',\n",
       " 'inasmuch',\n",
       " 'inc',\n",
       " 'indeed',\n",
       " 'indicate',\n",
       " 'indicated',\n",
       " 'indicates',\n",
       " 'inner',\n",
       " 'insofar',\n",
       " 'instead',\n",
       " 'into',\n",
       " 'inward',\n",
       " 'is',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'j',\n",
       " 'just',\n",
       " 'k',\n",
       " 'keep',\n",
       " 'keeps',\n",
       " 'kept',\n",
       " 'know',\n",
       " 'known',\n",
       " 'knows',\n",
       " 'l',\n",
       " 'last',\n",
       " 'lately',\n",
       " 'later',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'lest',\n",
       " 'let',\n",
       " \"let's\",\n",
       " 'like',\n",
       " 'liked',\n",
       " 'likely',\n",
       " 'little',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'ltd',\n",
       " 'm',\n",
       " 'mainly',\n",
       " 'many',\n",
       " 'may',\n",
       " 'maybe',\n",
       " 'me',\n",
       " 'mean',\n",
       " 'meanwhile',\n",
       " 'merely',\n",
       " 'might',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'n',\n",
       " 'name',\n",
       " 'namely',\n",
       " 'nd',\n",
       " 'near',\n",
       " 'nearly',\n",
       " 'necessary',\n",
       " 'need',\n",
       " 'needs',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'new',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'non',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'normally',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'novel',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'o',\n",
       " 'obviously',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'ones',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'ought',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'outside',\n",
       " 'over',\n",
       " 'overall',\n",
       " 'own',\n",
       " 'p',\n",
       " 'particular',\n",
       " 'particularly',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'placed',\n",
       " 'please',\n",
       " 'plus',\n",
       " 'possible',\n",
       " 'presumably',\n",
       " 'probably',\n",
       " 'provides',\n",
       " 'q',\n",
       " 'que',\n",
       " 'quite',\n",
       " 'qv',\n",
       " 'r',\n",
       " 'rather',\n",
       " 'rd',\n",
       " 're',\n",
       " 'really',\n",
       " 'reasonably',\n",
       " 'regarding',\n",
       " 'regardless',\n",
       " 'regards',\n",
       " 'relatively',\n",
       " 'respectively',\n",
       " 'right',\n",
       " 's',\n",
       " 'said',\n",
       " 'same',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'second',\n",
       " 'secondly',\n",
       " 'see',\n",
       " 'seeing',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'seen',\n",
       " 'self',\n",
       " 'selves',\n",
       " 'sensible',\n",
       " 'sent',\n",
       " 'serious',\n",
       " 'seriously',\n",
       " 'seven',\n",
       " 'several',\n",
       " 'shall',\n",
       " 'she',\n",
       " 'should',\n",
       " \"shouldn't\",\n",
       " 'since',\n",
       " 'six',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somebody',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhat',\n",
       " 'somewhere',\n",
       " 'soon',\n",
       " 'sorry',\n",
       " 'specified',\n",
       " 'specify',\n",
       " 'specifying',\n",
       " 'still',\n",
       " 'sub',\n",
       " 'such',\n",
       " 'sup',\n",
       " 'sure',\n",
       " 't',\n",
       " \"t's\",\n",
       " 'take',\n",
       " 'taken',\n",
       " 'tell',\n",
       " 'tends',\n",
       " 'th',\n",
       " 'than',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'thanx',\n",
       " 'that',\n",
       " \"that's\",\n",
       " 'thats',\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " \"there's\",\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'theres',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'think',\n",
       " 'third',\n",
       " 'this',\n",
       " 'thorough',\n",
       " 'thoroughly',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'took',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'tried',\n",
       " 'tries',\n",
       " 'truly',\n",
       " 'try',\n",
       " 'trying',\n",
       " 'twice',\n",
       " 'two',\n",
       " 'u',\n",
       " 'un',\n",
       " 'under',\n",
       " 'unfortunately',\n",
       " 'unless',\n",
       " 'unlikely',\n",
       " 'until',\n",
       " 'unto',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'useful',\n",
       " 'uses',\n",
       " 'using',\n",
       " 'usually',\n",
       " 'uucp',\n",
       " 'v',\n",
       " 'value',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'viz',\n",
       " 'vs',\n",
       " 'w',\n",
       " 'want',\n",
       " 'wants',\n",
       " 'was',\n",
       " \"wasn't\",\n",
       " 'way',\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'welcome',\n",
       " 'well',\n",
       " 'went',\n",
       " 'were',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " \"what's\",\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " \"where's\",\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " \"who's\",\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'willing',\n",
       " 'wish',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " \"won't\",\n",
       " 'wonder',\n",
       " 'would',\n",
       " \"wouldn't\",\n",
       " 'x',\n",
       " 'y',\n",
       " 'yes',\n",
       " 'yet',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'z',\n",
       " 'zero'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(token_list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_token = []\n",
    "    for each in token_list :\n",
    "#         print(each ,\":\", lemmatizer.lemmatize(each)) \n",
    "        lem_token.append(lemmatizer.lemmatize(each))\n",
    "    return lem_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "def fix_contractions(text):\n",
    "    return contractions.fix(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"\\w+(?:[']\\w+)?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(raw_data):\n",
    "    raw_data_low = raw_data.lower()\n",
    "#     raw_data1 = fix_contractions(raw_data_low)\n",
    "    tokenised = tokenizer.tokenize(raw_data_low)\n",
    "#     tokenised = nltk.tokenize.word_tokenize(raw_data1)\n",
    "#     lem_token = lemmatization(tokenised)\n",
    "#     stopwords_tokens = [w for w in tokenised if not w in stopwords]\n",
    "    processed_data = ' '.join(tokenised)\n",
    "        \n",
    "    return(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lab_data['text'] = lab_data.apply(lambda row: token(row['text'].strip()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"flirted with giving this two stars but that's a pretty damning rating for what might have just been an off night new to the east side and so we don't know many of these hidden gems but me and the fiance met her friend for drinks here and ended up getting some things to nibble first off service was pretty slow which was unusual because the restaurant is pretty small and galley style you would think it would be easy for servers to routinely hit up tables as you pass by the fiance ordered the quinoa salad and said it was pretty good but dry i wasn't too hungry and so i simply ordered the bruchetta way which came with burnt crostinis and i ordered a side of fries which were either hard or chewy the friend ordered the macaroni cheese and added chicken and bacon her usual order and liked it can't remember the last time i thought to myself huh they failed at fries so like i said two stars but the decor was good it was a good place to have a conversation and i might be back to try more expensive fare but ah the fry thing yeeesh i dunno man\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_data['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for each in lab_data['text']:\n",
    "    tokens = each.split()\n",
    "    sentences.extend(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'new',\n",
       " 'rule',\n",
       " 'is',\n",
       " 'if',\n",
       " 'you',\n",
       " 'are',\n",
       " 'waiting',\n",
       " 'for',\n",
       " 'a',\n",
       " 'table',\n",
       " 'which',\n",
       " 'you',\n",
       " 'almost',\n",
       " 'always',\n",
       " 'are',\n",
       " 'you',\n",
       " 'cant',\n",
       " 'wait',\n",
       " 'inside',\n",
       " 'they',\n",
       " 'just',\n",
       " 'posted',\n",
       " 'a',\n",
       " 'sign',\n",
       " 'upfront',\n",
       " 'that',\n",
       " 'it',\n",
       " 'causes',\n",
       " 'some',\n",
       " 'concerns',\n",
       " 'for',\n",
       " 'the',\n",
       " 'seated',\n",
       " 'patrons',\n",
       " 'how',\n",
       " 'awful',\n",
       " 'is',\n",
       " 'that',\n",
       " 'i',\n",
       " 'like',\n",
       " 'that',\n",
       " 'they',\n",
       " 'included',\n",
       " 'the',\n",
       " 'apology',\n",
       " 'along',\n",
       " 'with',\n",
       " 'especially',\n",
       " 'now',\n",
       " 'in',\n",
       " 'the',\n",
       " 'cold',\n",
       " 'p',\n",
       " 's',\n",
       " 'you',\n",
       " 'can',\n",
       " 'try',\n",
       " 'calling',\n",
       " 'in',\n",
       " 'ahead',\n",
       " 'to',\n",
       " 'reserve',\n",
       " 'a',\n",
       " 'table',\n",
       " 'but',\n",
       " 'thats',\n",
       " 'only',\n",
       " 'if',\n",
       " 'the',\n",
       " 'waiting',\n",
       " 'list',\n",
       " 'is',\n",
       " 'short',\n",
       " 'otherwise',\n",
       " 'you',\n",
       " 'have',\n",
       " 'to',\n",
       " 'show',\n",
       " 'up',\n",
       " 'to',\n",
       " 'reserve',\n",
       " 'boourns',\n",
       " 'this',\n",
       " 'place',\n",
       " 'could',\n",
       " 'do',\n",
       " 'no',\n",
       " 'wrong',\n",
       " 'in',\n",
       " 'my',\n",
       " 'eyes',\n",
       " 'rattle',\n",
       " 'away',\n",
       " 'you',\n",
       " 'equally',\n",
       " 'clever',\n",
       " 'witty',\n",
       " 'name',\n",
       " 'for',\n",
       " 'a',\n",
       " 'hot',\n",
       " 'beverage',\n",
       " 'must',\n",
       " 'mention',\n",
       " 'i',\n",
       " 'am',\n",
       " 'obsessed',\n",
       " 'with',\n",
       " 'mad',\n",
       " 'gab',\n",
       " 'xoxom',\n",
       " 'flirted',\n",
       " 'with',\n",
       " 'giving',\n",
       " 'this',\n",
       " 'two',\n",
       " 'stars',\n",
       " 'but',\n",
       " \"that's\",\n",
       " 'a',\n",
       " 'pretty',\n",
       " 'damning',\n",
       " 'rating',\n",
       " 'for',\n",
       " 'what',\n",
       " 'might',\n",
       " 'have',\n",
       " 'just',\n",
       " 'been',\n",
       " 'an',\n",
       " 'off',\n",
       " 'night',\n",
       " 'new',\n",
       " 'to',\n",
       " 'the',\n",
       " 'east',\n",
       " 'side',\n",
       " 'and',\n",
       " 'so',\n",
       " 'we',\n",
       " \"don't\",\n",
       " 'know',\n",
       " 'many',\n",
       " 'of',\n",
       " 'these',\n",
       " 'hidden',\n",
       " 'gems',\n",
       " 'but',\n",
       " 'me',\n",
       " 'and',\n",
       " 'the',\n",
       " 'fiance',\n",
       " 'met',\n",
       " 'her',\n",
       " 'friend',\n",
       " 'for',\n",
       " 'drinks',\n",
       " 'here',\n",
       " 'and',\n",
       " 'ended',\n",
       " 'up',\n",
       " 'getting',\n",
       " 'some',\n",
       " 'things',\n",
       " 'to',\n",
       " 'nibble',\n",
       " 'first',\n",
       " 'off',\n",
       " 'service',\n",
       " 'was',\n",
       " 'pretty',\n",
       " 'slow',\n",
       " 'which',\n",
       " 'was',\n",
       " 'unusual',\n",
       " 'because',\n",
       " 'the',\n",
       " 'restaurant',\n",
       " 'is',\n",
       " 'pretty',\n",
       " 'small',\n",
       " 'and',\n",
       " 'galley',\n",
       " 'style',\n",
       " 'you',\n",
       " 'would',\n",
       " 'think',\n",
       " 'it',\n",
       " 'would',\n",
       " 'be',\n",
       " 'easy',\n",
       " 'for',\n",
       " 'servers',\n",
       " 'to',\n",
       " 'routinely',\n",
       " 'hit',\n",
       " 'up',\n",
       " 'tables',\n",
       " 'as',\n",
       " 'you',\n",
       " 'pass',\n",
       " 'by',\n",
       " 'the',\n",
       " 'fiance',\n",
       " 'ordered',\n",
       " 'the',\n",
       " 'quinoa',\n",
       " 'salad',\n",
       " 'and',\n",
       " 'said',\n",
       " 'it',\n",
       " 'was',\n",
       " 'pretty',\n",
       " 'good',\n",
       " 'but',\n",
       " 'dry',\n",
       " 'i',\n",
       " \"wasn't\",\n",
       " 'too',\n",
       " 'hungry',\n",
       " 'and',\n",
       " 'so',\n",
       " 'i',\n",
       " 'simply',\n",
       " 'ordered',\n",
       " 'the',\n",
       " 'bruchetta',\n",
       " 'way',\n",
       " 'which',\n",
       " 'came',\n",
       " 'with',\n",
       " 'burnt',\n",
       " 'crostinis',\n",
       " 'and',\n",
       " 'i',\n",
       " 'ordered',\n",
       " 'a',\n",
       " 'side',\n",
       " 'of',\n",
       " 'fries',\n",
       " 'which',\n",
       " 'were',\n",
       " 'either',\n",
       " 'hard',\n",
       " 'or',\n",
       " 'chewy',\n",
       " 'the',\n",
       " 'friend',\n",
       " 'ordered',\n",
       " 'the',\n",
       " 'macaroni',\n",
       " 'cheese',\n",
       " 'and',\n",
       " 'added',\n",
       " 'chicken',\n",
       " 'and',\n",
       " 'bacon',\n",
       " 'her',\n",
       " 'usual',\n",
       " 'order',\n",
       " 'and',\n",
       " 'liked',\n",
       " 'it',\n",
       " \"can't\",\n",
       " 'remember',\n",
       " 'the',\n",
       " 'last',\n",
       " 'time',\n",
       " 'i',\n",
       " 'thought',\n",
       " 'to',\n",
       " 'myself',\n",
       " 'huh',\n",
       " 'they',\n",
       " 'failed',\n",
       " 'at',\n",
       " 'fries',\n",
       " 'so',\n",
       " 'like',\n",
       " 'i',\n",
       " 'said',\n",
       " 'two',\n",
       " 'stars',\n",
       " 'but',\n",
       " 'the',\n",
       " 'decor',\n",
       " 'was',\n",
       " 'good',\n",
       " 'it',\n",
       " 'was',\n",
       " 'a',\n",
       " 'good',\n",
       " 'place',\n",
       " 'to',\n",
       " 'have',\n",
       " 'a',\n",
       " 'conversation',\n",
       " 'and',\n",
       " 'i',\n",
       " 'might',\n",
       " 'be',\n",
       " 'back',\n",
       " 'to',\n",
       " 'try',\n",
       " 'more',\n",
       " 'expensive',\n",
       " 'fare',\n",
       " 'but',\n",
       " 'ah',\n",
       " 'the',\n",
       " 'fry',\n",
       " 'thing',\n",
       " 'yeeesh',\n",
       " 'i',\n",
       " 'dunno',\n",
       " 'man',\n",
       " 'i',\n",
       " 'was',\n",
       " 'staying',\n",
       " 'at',\n",
       " 'planet',\n",
       " 'hollywood',\n",
       " 'across',\n",
       " 'the',\n",
       " 'street',\n",
       " 'and',\n",
       " 'saw',\n",
       " 'good',\n",
       " 'reviews',\n",
       " 'on',\n",
       " 'this',\n",
       " 'place',\n",
       " 'so',\n",
       " 'my',\n",
       " 'husband',\n",
       " 'and',\n",
       " 'i',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'give',\n",
       " 'it',\n",
       " 'a',\n",
       " 'try',\n",
       " 'we',\n",
       " 'love',\n",
       " 'a',\n",
       " 'good',\n",
       " 'breakfast',\n",
       " 'sandwich',\n",
       " 'we',\n",
       " 'both',\n",
       " 'ordered',\n",
       " 'the',\n",
       " 'bacon',\n",
       " 'egg',\n",
       " 'and',\n",
       " 'cheese',\n",
       " 'cold',\n",
       " 'brew',\n",
       " 'a',\n",
       " 'biscuit',\n",
       " 'and',\n",
       " 'a',\n",
       " 'salted',\n",
       " 'caramel',\n",
       " 'cookie',\n",
       " 'everything',\n",
       " 'tasted',\n",
       " 'amazing',\n",
       " 'the',\n",
       " 'egg',\n",
       " 'is',\n",
       " 'medium',\n",
       " 'so',\n",
       " 'it',\n",
       " 'pops',\n",
       " 'when',\n",
       " 'you',\n",
       " 'take',\n",
       " 'a',\n",
       " 'bite',\n",
       " 'so',\n",
       " 'be',\n",
       " 'prepared',\n",
       " 'for',\n",
       " 'that',\n",
       " 'the',\n",
       " 'cookie',\n",
       " 'was',\n",
       " 'absolutely',\n",
       " 'delicious',\n",
       " 'and',\n",
       " 'the',\n",
       " 'biscuit',\n",
       " 'was',\n",
       " 'soft',\n",
       " 'and',\n",
       " 'fluffy',\n",
       " 'we',\n",
       " 'got',\n",
       " 'there',\n",
       " 'are',\n",
       " 'around',\n",
       " 'am',\n",
       " 'and',\n",
       " 'there',\n",
       " 'was',\n",
       " 'a',\n",
       " 'line',\n",
       " 'already',\n",
       " 'luckily',\n",
       " 'the',\n",
       " 'line',\n",
       " 'moves',\n",
       " 'very',\n",
       " 'quick',\n",
       " 'and',\n",
       " 'we',\n",
       " 'were',\n",
       " 'able',\n",
       " 'to',\n",
       " 'find',\n",
       " 'seating',\n",
       " 'highly',\n",
       " 'recommend',\n",
       " 'this',\n",
       " 'place',\n",
       " 'food',\n",
       " 'is',\n",
       " 'good',\n",
       " 'but',\n",
       " 'prices',\n",
       " 'are',\n",
       " 'super',\n",
       " 'expensive',\n",
       " 'bucks',\n",
       " 'for',\n",
       " 'the',\n",
       " 'extra',\n",
       " 'large',\n",
       " 'carne',\n",
       " 'asada',\n",
       " 'burrito',\n",
       " 'and',\n",
       " \"it's\",\n",
       " 'a',\n",
       " 'little',\n",
       " 'bigger',\n",
       " 'then',\n",
       " 'a',\n",
       " 'taco',\n",
       " 'bell',\n",
       " 'bean',\n",
       " 'burrito',\n",
       " \"don't\",\n",
       " 'get',\n",
       " 'me',\n",
       " 'wrong',\n",
       " \"it's\",\n",
       " 'good',\n",
       " 'carne',\n",
       " 'asada',\n",
       " 'but',\n",
       " 'you',\n",
       " 'can',\n",
       " 'get',\n",
       " 'one',\n",
       " 'at',\n",
       " 'filibertos',\n",
       " 'and',\n",
       " \"it's\",\n",
       " 'literally',\n",
       " 'three',\n",
       " 'times',\n",
       " 'the',\n",
       " 'size',\n",
       " 'for',\n",
       " 'bucks',\n",
       " 'lower',\n",
       " 'your',\n",
       " 'prices',\n",
       " 'or',\n",
       " 'make',\n",
       " 'the',\n",
       " 'portions',\n",
       " 'bigger',\n",
       " \"don't\",\n",
       " 'forget',\n",
       " 'that',\n",
       " \"you're\",\n",
       " 'using',\n",
       " 'the',\n",
       " 'cheapest',\n",
       " 'cut',\n",
       " 'of',\n",
       " 'beef',\n",
       " 'you',\n",
       " 'can',\n",
       " 'buy',\n",
       " 'marinating',\n",
       " 'it',\n",
       " 'with',\n",
       " 'some',\n",
       " 'orange',\n",
       " 'juice',\n",
       " 'garlic',\n",
       " 'and',\n",
       " 'onions',\n",
       " 'and',\n",
       " 'then',\n",
       " 'grilling',\n",
       " 'it',\n",
       " \"we're\",\n",
       " 'not',\n",
       " 'talking',\n",
       " 'foie',\n",
       " 'gras',\n",
       " 'and',\n",
       " 'heirloom',\n",
       " 'tomatoes',\n",
       " 'and',\n",
       " 'seriously',\n",
       " 'one',\n",
       " 'piece',\n",
       " 'of',\n",
       " 'cucumber',\n",
       " 'and',\n",
       " 'a',\n",
       " 'quarter',\n",
       " 'of',\n",
       " 'a',\n",
       " 'radish',\n",
       " \"i've\",\n",
       " 'gone',\n",
       " 'there',\n",
       " 'or',\n",
       " 'five',\n",
       " 'times',\n",
       " 'and',\n",
       " 'each',\n",
       " 'time',\n",
       " 'do',\n",
       " \"i'm\",\n",
       " 'left',\n",
       " 'with',\n",
       " 'the',\n",
       " 'same',\n",
       " 'opinion',\n",
       " 'so',\n",
       " 'here',\n",
       " 'it',\n",
       " 'is',\n",
       " 'publicly',\n",
       " 'worse',\n",
       " 'company',\n",
       " 'to',\n",
       " 'deal',\n",
       " 'with',\n",
       " 'they',\n",
       " 'do',\n",
       " 'horrible',\n",
       " 'work',\n",
       " 'had',\n",
       " 'to',\n",
       " 'bring',\n",
       " 'my',\n",
       " 'truck',\n",
       " 'back',\n",
       " 'on',\n",
       " 'a',\n",
       " 'door',\n",
       " 'replacement',\n",
       " 'the',\n",
       " 'door',\n",
       " 'paint',\n",
       " \"didn't\",\n",
       " 'match',\n",
       " 'and',\n",
       " 'trim',\n",
       " 'molding',\n",
       " \"wasn't\",\n",
       " 'straight',\n",
       " 'the',\n",
       " 'bolt',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'the',\n",
       " 'door',\n",
       " 'on',\n",
       " \"wasn't\",\n",
       " 'even',\n",
       " 'tight',\n",
       " 'my',\n",
       " 'speaker',\n",
       " 'in',\n",
       " 'the',\n",
       " 'door',\n",
       " \"wasn't\",\n",
       " 'hooked',\n",
       " 'back',\n",
       " 'up',\n",
       " 'i',\n",
       " \"don't\",\n",
       " 'recommend',\n",
       " 'anyone',\n",
       " 'to',\n",
       " 'bring',\n",
       " 'the',\n",
       " 'vehicle',\n",
       " 'here',\n",
       " 'a',\n",
       " 'gentleman',\n",
       " 'by',\n",
       " 'the',\n",
       " 'name',\n",
       " 'of',\n",
       " 'jesus',\n",
       " 'that',\n",
       " 'works',\n",
       " 'there',\n",
       " 'every',\n",
       " 'time',\n",
       " 'you',\n",
       " 'call',\n",
       " 'for',\n",
       " 'an',\n",
       " 'update',\n",
       " 'because',\n",
       " 'they',\n",
       " 'say',\n",
       " 'they',\n",
       " 'will',\n",
       " 'call',\n",
       " 'you',\n",
       " 'and',\n",
       " 'never',\n",
       " 'do',\n",
       " 'he',\n",
       " 'makes',\n",
       " 'you',\n",
       " 'feel',\n",
       " 'like',\n",
       " 'your',\n",
       " 'bothering',\n",
       " 'him',\n",
       " 'and',\n",
       " 'then',\n",
       " 'he',\n",
       " 'says',\n",
       " 'he',\n",
       " 'will',\n",
       " 'call',\n",
       " 'you',\n",
       " 'back',\n",
       " 'with',\n",
       " 'an',\n",
       " 'update',\n",
       " 'and',\n",
       " 'never',\n",
       " 'do',\n",
       " 'if',\n",
       " 'you',\n",
       " 'need',\n",
       " 'work',\n",
       " 'on',\n",
       " 'your',\n",
       " 'vehicle',\n",
       " 'i',\n",
       " 'would',\n",
       " 'double',\n",
       " 'think',\n",
       " 'it',\n",
       " 'before',\n",
       " 'you',\n",
       " 'bring',\n",
       " 'it',\n",
       " 'here',\n",
       " 'i',\n",
       " 'hate',\n",
       " 'mcdonalds',\n",
       " 'food',\n",
       " 'however',\n",
       " 'i',\n",
       " 'had',\n",
       " 'to',\n",
       " 'pee',\n",
       " 'and',\n",
       " 'i',\n",
       " \"didn't\",\n",
       " 'know',\n",
       " 'where',\n",
       " 'else',\n",
       " 'to',\n",
       " 'go',\n",
       " 'i',\n",
       " 'bought',\n",
       " 'a',\n",
       " 'smoothie',\n",
       " 'which',\n",
       " 'was',\n",
       " 'surprisingly',\n",
       " 'good',\n",
       " 'actually',\n",
       " 'the',\n",
       " 'girl',\n",
       " 'who',\n",
       " 'waited',\n",
       " 'on',\n",
       " 'me',\n",
       " 'was',\n",
       " 'super',\n",
       " 'nice',\n",
       " 'a',\n",
       " 'solid',\n",
       " 'breakfast',\n",
       " 'but',\n",
       " 'a',\n",
       " 'long',\n",
       " 'wait',\n",
       " 'overall',\n",
       " \"it's\",\n",
       " 'overrated',\n",
       " 'at',\n",
       " 'the',\n",
       " 'end',\n",
       " 'of',\n",
       " 'the',\n",
       " 'day',\n",
       " 'they',\n",
       " 'have',\n",
       " 'eggs',\n",
       " 'and',\n",
       " 'bacon',\n",
       " 'and',\n",
       " \"there's\",\n",
       " 'nothing',\n",
       " 'much',\n",
       " 'special',\n",
       " 'about',\n",
       " 'that',\n",
       " 'due',\n",
       " 'to',\n",
       " 'misunderstanding',\n",
       " 'between',\n",
       " 'my',\n",
       " 'insurance',\n",
       " 'company',\n",
       " 'and',\n",
       " 'crs',\n",
       " 'temporary',\n",
       " 'housing',\n",
       " 'they',\n",
       " 'send',\n",
       " 'me',\n",
       " 'a',\n",
       " 'collection',\n",
       " 'letter',\n",
       " 'with',\n",
       " 'falsified',\n",
       " 'attachments',\n",
       " 'it',\n",
       " 'was',\n",
       " 'a',\n",
       " 'contract',\n",
       " 'not',\n",
       " 'even',\n",
       " 'related',\n",
       " 'to',\n",
       " 'my',\n",
       " 'claim',\n",
       " 'very',\n",
       " 'unprofessional',\n",
       " 'very',\n",
       " 'sneaky',\n",
       " 'very',\n",
       " 'illegal',\n",
       " 'i',\n",
       " 'could',\n",
       " 'go',\n",
       " 'with',\n",
       " 'either',\n",
       " 'two',\n",
       " 'or',\n",
       " 'three',\n",
       " 'stars',\n",
       " 'restaurant',\n",
       " 'was',\n",
       " 'full',\n",
       " 'time',\n",
       " 'from',\n",
       " 'seating',\n",
       " 'to',\n",
       " 'food',\n",
       " 'arrival',\n",
       " 'was',\n",
       " 'minutes',\n",
       " 'i',\n",
       " 'had',\n",
       " 'the',\n",
       " 'spinach',\n",
       " 'crepe',\n",
       " 'which',\n",
       " 'was',\n",
       " 'bland',\n",
       " 'the',\n",
       " 'potato',\n",
       " 'pancakes',\n",
       " 'that',\n",
       " 'came',\n",
       " 'with',\n",
       " 'it',\n",
       " 'were',\n",
       " 'too',\n",
       " 'sour',\n",
       " 'kitchen',\n",
       " 'made',\n",
       " 'the',\n",
       " 'wrong',\n",
       " 'dish',\n",
       " 'for',\n",
       " 'two',\n",
       " 'of',\n",
       " 'the',\n",
       " \"table's\",\n",
       " 'orders',\n",
       " 'options',\n",
       " 'were',\n",
       " 'to',\n",
       " 'accept',\n",
       " 'incorrect',\n",
       " 'food',\n",
       " 'or',\n",
       " 'wait',\n",
       " 'even',\n",
       " 'longer',\n",
       " 'for',\n",
       " 'correct',\n",
       " 'food',\n",
       " 'to',\n",
       " 'be',\n",
       " 'made',\n",
       " 'ugh',\n",
       " 'to',\n",
       " 'top',\n",
       " 'it',\n",
       " 'off',\n",
       " 'when',\n",
       " 'i',\n",
       " 'went',\n",
       " 'to',\n",
       " 'use',\n",
       " 'the',\n",
       " 'restroom',\n",
       " 'they',\n",
       " 'were',\n",
       " 'out',\n",
       " 'of',\n",
       " 'toilet',\n",
       " 'paper',\n",
       " 'things',\n",
       " 'to',\n",
       " 'know',\n",
       " 'they',\n",
       " 'are',\n",
       " 'closed',\n",
       " 'on',\n",
       " 'tuesdays',\n",
       " 'no',\n",
       " 'credit',\n",
       " 'cards',\n",
       " 'omelets',\n",
       " 'come',\n",
       " 'with',\n",
       " 'pancakes',\n",
       " 'unless',\n",
       " 'you',\n",
       " 'specify',\n",
       " 'toast',\n",
       " 'and',\n",
       " 'omelets',\n",
       " 'are',\n",
       " 'four',\n",
       " 'egg',\n",
       " 'this',\n",
       " 'is',\n",
       " 'all',\n",
       " 'stated',\n",
       " 'on',\n",
       " 'the',\n",
       " 'menu',\n",
       " 'etc',\n",
       " 'but',\n",
       " 'people',\n",
       " 'still',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'miss',\n",
       " 'them',\n",
       " 'oh',\n",
       " 'and',\n",
       " 'the',\n",
       " 'online',\n",
       " 'menu',\n",
       " 'is',\n",
       " 'different',\n",
       " 'than',\n",
       " 'the',\n",
       " 'actual',\n",
       " 'menu',\n",
       " 'some',\n",
       " 'items',\n",
       " 'are',\n",
       " 'missing',\n",
       " 'but',\n",
       " 'the',\n",
       " 'in',\n",
       " 'person',\n",
       " 'menu',\n",
       " 'is',\n",
       " 'much',\n",
       " 'larger',\n",
       " 'i',\n",
       " 'stayed',\n",
       " 'here',\n",
       " 'so',\n",
       " 'often',\n",
       " 'that',\n",
       " 'they',\n",
       " 'knew',\n",
       " 'my',\n",
       " 'name',\n",
       " 'and',\n",
       " 'often',\n",
       " 'gave',\n",
       " 'me',\n",
       " 'the',\n",
       " 'same',\n",
       " 'room',\n",
       " 'the',\n",
       " 'hotel',\n",
       " 'is',\n",
       " 'downtown',\n",
       " 'has',\n",
       " 'views',\n",
       " 'of',\n",
       " 'the',\n",
       " 'lake',\n",
       " 'and',\n",
       " 'the',\n",
       " 'rooms',\n",
       " 'and',\n",
       " 'their',\n",
       " 'beds',\n",
       " 'are',\n",
       " 'well',\n",
       " 'heavenly',\n",
       " 'very',\n",
       " 'decent',\n",
       " 'exercise',\n",
       " 'room',\n",
       " 'superb',\n",
       " 'bar',\n",
       " 'there',\n",
       " 'have',\n",
       " 'been',\n",
       " 'some',\n",
       " 'problems',\n",
       " 'of',\n",
       " 'late',\n",
       " 'with',\n",
       " 'mold',\n",
       " 'contact',\n",
       " 'then',\n",
       " 'first',\n",
       " 'to',\n",
       " 'see',\n",
       " 'what',\n",
       " 'they',\n",
       " 'have',\n",
       " 'to',\n",
       " 'say',\n",
       " 'about',\n",
       " 'it',\n",
       " 'decent',\n",
       " 'cocktail',\n",
       " 'annoying',\n",
       " 'bartender',\n",
       " 'and',\n",
       " 'food',\n",
       " 'was',\n",
       " 'not',\n",
       " 'good',\n",
       " 'i',\n",
       " 'had',\n",
       " 'the',\n",
       " 'chicken',\n",
       " 'biscuits',\n",
       " 'and',\n",
       " 'the',\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "import collections\n",
    "\n",
    "def get_bigram_without_stopwords(sentences):\n",
    "    all_bigram = ngrams(sentences, 2)\n",
    "    bigram_freq = collections.Counter(all_bigram)\n",
    "    bigram_1000 = bigram_freq.most_common(5000)\n",
    "    bigram_wo_stop = [(bigram[0],bigram[1]) for bigram, freq in bigram_1000 \n",
    "              if (bigram[0].lower() not in stopwords and bigram[1].lower() not in stopwords)]\n",
    "    bigram_wo_stop = bigram_wo_stop[0:400]\n",
    "    return bigram_wo_stop\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_wo_stop_1 = get_bigram_without_stopwords(sentences)\n",
    "mwe_tokenizer = MWETokenizer(bigram_wo_stop_1, separator='_')\n",
    "\n",
    "def generate_bigram(raw_text, mwe_tokenizer):\n",
    "    tokens = raw_text.split()\n",
    "    tokens_b = mwe_tokenizer.tokenize(tokens)\n",
    "    lem_token = lemmatization(tokens_b)\n",
    "    processed_data = ' '.join(tokens_b)\n",
    "    return processed_data\n",
    "\n",
    "lab_data['text'] = lab_data.apply(lambda row: generate_bigram(row['text'].strip(), mwe_tokenizer), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the new rule is if you are waiting for a table which you almost always are you cant wait inside they just posted a sign upfront that it causes some concerns for the seated patrons how awful is that i like that they included the apology along with especially now in the cold p s you can try calling in ahead to reserve a table but thats only if the waiting list is short otherwise you have to show up to reserve boourns this place could do no wrong in my eyes rattle away you equally clever witty name for a hot beverage must mention i am obsessed with mad gab xoxom'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_data['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase = True,analyzer = 'word',ngram_range = (1,1), min_df=3, max_df=.99)\n",
    "    \n",
    "train_review = vectorizer.fit_transform(lab_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_review, lab_data['label'],test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_cross_val(model):\n",
    "    # perfroming 10 fold cross validation\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    params = {}\n",
    "    nb = model\n",
    "    gs = GridSearchCV(nb, cv=skf, param_grid=params, return_train_score=False)\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "gs = instantiate_cross_val(model)\n",
    "\n",
    "clf=gs.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('parameters:', clf.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegression(random_state=1, C=1, solver='sag', multi_class = 'multinomial')\n",
    "log_model.fit(X_train, y_train)\n",
    "y_pred = log_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy: 0.6123 2g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regularization penalty space\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# Create regularization hyperparameter space\n",
    "C = np.logspace(0, 4, 10)\n",
    "\n",
    "# multi_class = ['multinomial','ovr']\n",
    "\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(model, hyperparameters, cv=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View best hyperparameters\n",
    "print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best C:', best_model.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_best_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_best_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "# Perform classification with SVM, kernel=linear\n",
    "classifier_linear = svm.SVC(kernel='linear')\n",
    "classifier_linear.fit(X_train, y_train)\n",
    "prediction_linear = classifier_linear.predict(X_test)\n",
    "# results\n",
    "classification_report(y_test, prediction_linear, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.accuracy_score( y_test,prediction_linear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X_data, y_data, batch_size):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    index = np.arange(np.shape(y_data)[0])\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:].toarray()\n",
    "        y_batch = y_data[y_data.index[index_batch]]\n",
    "        counter += 1\n",
    "        yield X_batch,y_batch\n",
    "        if (counter > number_of_batches):\n",
    "            counter=0\n",
    "            \n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(generator=batch_generator(X_train, y_train, 32),\n",
    "                    epochs=5, validation_data=(X_test, y_test),\n",
    "                    steps_per_epoch=X_train.shape[0]/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plot_confusion_matrix(cm, classes=['1', '2', '3', '4', '5'],\n",
    "                      title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probab = clf.predict_proba(test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pred_probab)):\n",
    "    p_test.append(max(pred_probab[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame({'text':X_test, 'label':y_test, 'p_test':p_test, 'y_pred':y_pred})\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data[(train_data['p_test'] > 0.9) & (train_data['label']==train_data['y_pred'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec + Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_data['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = []\n",
    "# for review in lab_data['text']:\n",
    "#     sentences.append(review.split(' '))\n",
    "sentences = lab_data.apply(lambda row: row['text'].split(), axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import phrases\n",
    "bigrams = phrases.Phrases(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bigrams[\"this is the new york\".split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 200    # Word vector dimensionality                      \n",
    "min_word_count = 10   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "# context = 10          # Context window size                                                                                    \n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(bigrams[sentences], workers=num_workers, \\\n",
    "            size=num_features, min_count=3)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "# model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?word2vec.Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "list(islice(model.wv.vocab, 11030, 13050))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(lab_data, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = text.split(' ')\n",
    "    return tokens\n",
    "\n",
    "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['text']), axis=1).values\n",
    "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['text']), axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_tokenized = test['text'].values\n",
    "train_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, review) for review in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_word_average = word_averaging_list(model.wv,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(model.wv,test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=1, solver='liblinear', multi_class = 'ovr')\n",
    "logreg.fit(X_train_word_average, train['label'])\n",
    "y_pred = logreg.predict(X_test_word_average)\n",
    "print('accuracy %s' % metrics.accuracy_score(y_pred, test.label))\n",
    "# print(classification_report(test.label, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
