{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import nltk\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_data = pd.read_csv('labeled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_characters(raw_text):\n",
    "    processed_text = re.sub('\\\\n','', raw_text)\n",
    "    processed_text = re.sub('\\\\r','', processed_text)\n",
    "    processed_text = re.sub(\"\\\\'\", \"\\'\",processed_text)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_data['text'] = lab_data.apply(lambda row: remove_extra_characters(row['text'].strip()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "with open('./stopwords_en.txt') as f:\n",
    "    stopwords = f.read().splitlines()\n",
    "stopwords = set(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(token_list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_token = []\n",
    "    for each in token_list :\n",
    "#         print(each ,\":\", lemmatizer.lemmatize(each)) \n",
    "        lem_token.append(lemmatizer.lemmatize(each))\n",
    "    return lem_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"\\w+(?:[']\\w+)?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(raw_data):\n",
    " #   raw_data1 = raw_data.lower()\n",
    "    tokenised = tokenizer.tokenize(raw_data)\n",
    "#     tokenised = nltk.tokenize.word_tokenize(raw_data1)\n",
    "    lem_token = lemmatization(tokenised)\n",
    "#     stopwords_tokens = [w for w in tokenised if not w in stopwords]\n",
    "    processed_data = ' '.join(lem_token)\n",
    "        \n",
    "    return(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_data['text'] = lab_data.apply(lambda row: token(row['text'].strip()), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "all_words = []\n",
    "for sent in lab_data['text']:\n",
    "    tokenize_word = word_tokenize(sent)\n",
    "    for word in tokenize_word:\n",
    "        all_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74426\n"
     ]
    }
   ],
   "source": [
    "unique_words = set(all_words)\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = 7500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_sentences = [one_hot(sent, vocab_length) for sent in lab_data['text']]\n",
    "#print(embedded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = lambda sentence: len(word_tokenize(sentence))\n",
    "longest_sentence = max(lab_data['text'], key=word_count)\n",
    "length_long_sentence = len(word_tokenize(longest_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2305 3461  497 ...    0    0    0]\n",
      " [2673 2904 7484 ...    0    0    0]\n",
      " [3167 1736 1959 ...    0    0    0]\n",
      " ...\n",
      " [2434 2357 5971 ...    0    0    0]\n",
      " [5787 3167 3481 ...    0    0    0]\n",
      " [3573 6634 5312 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "padded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')\n",
    "print(padded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_length, 20, input_length=length_long_sentence))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss = 'categorical_crossentropy', metrics=['acc'])\n",
    "#print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "#print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_model.fit(x=train_word, y=train_y, epochs=1, batch_size=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments =  lab_data['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "sentiments = array(sentiments)\n",
    "print(type(sentiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "sentiments = to_categorical(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 14s 283us/step - loss: 0.4042 - acc: 0.8671\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 14s 286us/step - loss: 0.3236 - acc: 0.9009\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 16s 313us/step - loss: 0.2564 - acc: 0.9268\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 17s 335us/step - loss: 0.2011 - acc: 0.9478\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 16s 312us/step - loss: 0.1561 - acc: 0.9634\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 15s 306us/step - loss: 0.1201 - acc: 0.9751\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 16s 310us/step - loss: 0.0906 - acc: 0.9839\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 16s 325us/step - loss: 0.0671 - acc: 0.9895\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 16s 315us/step - loss: 0.0495 - acc: 0.9928\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 16s 319us/step - loss: 0.0355 - acc: 0.9961\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 16s 323us/step - loss: 0.0250 - acc: 0.9979\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 16s 316us/step - loss: 0.0175 - acc: 0.9989\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 16s 328us/step - loss: 0.0122 - acc: 0.9995\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 16s 323us/step - loss: 0.0085 - acc: 0.9997\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 16s 315us/step - loss: 0.0058 - acc: 0.9998\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 16s 324us/step - loss: 0.0044 - acc: 0.9999\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 17s 335us/step - loss: 0.0030 - acc: 0.9999\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 16s 328us/step - loss: 0.0022 - acc: 0.9999\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 16s 320us/step - loss: 0.0015 - acc: 0.9999\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 17s 344us/step - loss: 0.0011 - acc: 0.9999\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 17s 343us/step - loss: 9.3532e-04 - acc: 0.9999\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 17s 341us/step - loss: 0.0016 - acc: 0.9998\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 16s 327us/step - loss: 7.3129e-04 - acc: 0.9999\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 16s 327us/step - loss: 5.6269e-04 - acc: 0.9999\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 17s 338us/step - loss: 0.0011 - acc: 0.9998\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 17s 342us/step - loss: 6.1232e-04 - acc: 0.9999\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 17s 346us/step - loss: 6.5848e-04 - acc: 0.9999\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 17s 345us/step - loss: 5.0302e-04 - acc: 0.9999\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 17s 348us/step - loss: 3.1892e-04 - acc: 0.9999\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 17s 340us/step - loss: 5.1725e-04 - acc: 0.9999\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 17s 331us/step - loss: 3.9978e-04 - acc: 0.9999\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 17s 338us/step - loss: 2.8952e-04 - acc: 1.0000\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 17s 342us/step - loss: 3.0060e-04 - acc: 0.9999\n",
      "Epoch 34/50\n",
      "32160/50000 [==================>...........] - ETA: 6s - loss: 2.2265e-04 - acc: 1.0000"
     ]
    }
   ],
   "source": [
    "model.fit(padded_sentences, sentiments, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.792002\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(padded_sentences, sentiments, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss, accuracy = model.evaluate(padded_sentences, sentiments, verbose=0)\n",
    "#print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
